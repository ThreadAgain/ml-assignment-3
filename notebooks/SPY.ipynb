{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>LogRet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-09-20</td>\n",
       "      <td>1221.339966</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>-0.007895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-09-21</td>\n",
       "      <td>1210.199951</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.008379</td>\n",
       "      <td>-0.007895</td>\n",
       "      <td>0.094124</td>\n",
       "      <td>-0.009163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-09-22</td>\n",
       "      <td>1214.619995</td>\n",
       "      <td>-0.004003</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>-0.009163</td>\n",
       "      <td>-0.049652</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-09-23</td>\n",
       "      <td>1215.290039</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>-0.206151</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-09-26</td>\n",
       "      <td>1215.630005</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>6587.470215</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.008450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>6584.290039</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>-0.156219</td>\n",
       "      <td>-0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>6615.279785</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.004696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>2025-09-16</td>\n",
       "      <td>6606.759766</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.060471</td>\n",
       "      <td>-0.001289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>6600.350098</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>-0.007446</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>0.079906</td>\n",
       "      <td>-0.000971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5030 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Close      High       Low      Open    Volume  \\\n",
       "0     2005-09-20  1221.339966 -0.001148 -0.006194 -0.005581  0.110541   \n",
       "1     2005-09-21  1210.199951 -0.012181 -0.008379 -0.007895  0.094124   \n",
       "2     2005-09-22  1214.619995 -0.004003 -0.003759 -0.009163 -0.049652   \n",
       "3     2005-09-23  1215.290039  0.001798  0.003685  0.003646 -0.206151   \n",
       "4     2005-09-26  1215.630005  0.003056  0.001685  0.000551  0.024631   \n",
       "...          ...          ...       ...       ...       ...       ...   \n",
       "5025  2025-09-11  6587.470215  0.005616  0.004511  0.000629  0.032486   \n",
       "5026  2025-09-12  6584.290039  0.001110  0.005134  0.005515 -0.156219   \n",
       "5027  2025-09-15  6615.279785  0.002937  0.003426  0.001945  0.083334   \n",
       "5028  2025-09-16  6606.759766  0.001113 -0.000297  0.003121  0.060471   \n",
       "5029  2025-09-17  6600.350098 -0.000392 -0.007446 -0.002912  0.079906   \n",
       "\n",
       "        LogRet  \n",
       "0    -0.007895  \n",
       "1    -0.009163  \n",
       "2     0.003646  \n",
       "3     0.000551  \n",
       "4     0.000280  \n",
       "...        ...  \n",
       "5025  0.008450  \n",
       "5026 -0.000483  \n",
       "5027  0.004696  \n",
       "5028 -0.001289  \n",
       "5029 -0.000971  \n",
       "\n",
       "[5030 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/cleaned/sp500.parquet')\n",
    "df = df[0:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df.dropna(inplace=True)\n",
    "# Plot histogram of Close prices\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(df['LogRet'], kde=False, stat=\"density\", bins=30, label='LogRet Prices')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mean, std = norm.fit(df['LogRet'])\n",
    "x = np.linspace(df['LogRet'].min(), df['LogRet'].max(), 100)\n",
    "pdf = norm.pdf(x, mean, std)\n",
    "\n",
    "# Plot the normal distribution curve\n",
    "plt.plot(x, pdf, 'r-', label=f'Normal Fit (mean={mean:.2f}, std={std:.2f})')\n",
    "plt.title('Distribution of SP500 LogRet Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def stationarity_analysis(df, dataset_name, column_name = None):\n",
    "    \"\"\"\n",
    "    column_name: specify the column to analyze, default is 'Close'\n",
    "    Perform ADF and KPSS tests for stationarity analysis\n",
    "    \"\"\"\n",
    "    data = df['Close'] if column_name is None else df[column_name]\n",
    "    \n",
    "    data.dropna(inplace=True)  # Drop NaN values for accurate testing\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"STATIONARITY ANALYSIS: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # ADF Test\n",
    "    print(\"\\n--- AUGMENTED DICKEY-FULLER TEST ---\")\n",
    "    adf_result = adfuller(data, autolag='AIC')\n",
    "    print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
    "    print(f\"p-value: {adf_result[1]:.6f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"\\t{key}: {value:.6f}\")\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"ADF Result: STATIONARY (Reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"ADF Result: NON-STATIONARY (Fail to reject null hypothesis)\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    print(\"\\n--- KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN TEST ---\")\n",
    "    kpss_result = kpss(data, regression='c', nlags='auto')\n",
    "    print(f\"KPSS Statistic: {kpss_result[0]:.6f}\")\n",
    "    print(f\"p-value: {kpss_result[1]:.6f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f\"\\t{key}: {value:.6f}\")\n",
    "    \n",
    "    if kpss_result[1] <= 0.05:\n",
    "        print(\"KPSS Result: NON-STATIONARY (Reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"KPSS Result: STATIONARY (Fail to reject null hypothesis)\")\n",
    "    \n",
    "    # Combined interpretation\n",
    "    print(\"\\n--- COMBINED INTERPRETATION ---\")\n",
    "    adf_stationary = adf_result[1] <= 0.05\n",
    "    kpss_stationary = kpss_result[1] > 0.05\n",
    "    \n",
    "    if adf_stationary and kpss_stationary:\n",
    "        print(\"CONCLUSION: Series is STATIONARY\")\n",
    "    elif not adf_stationary and not kpss_stationary:\n",
    "        print(\"CONCLUSION: Series is NON-STATIONARY\")\n",
    "    else:\n",
    "        print(\"CONCLUSION: Results are INCONCLUSIVE - further investigation needed\")\n",
    "\n",
    "stationarity_analysis(df=df, dataset_name=\"S&P500_raw\")\n",
    "stationarity_analysis(df=df, dataset_name=\"S&P500_log_returns\", column_name='LogRet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr = df['LogRet'].mean()\n",
    "std_lr = df['LogRet'].std()\n",
    "CLAMP_STD = 3\n",
    "\n",
    "upper = mean_lr + CLAMP_STD * std_lr\n",
    "lower = mean_lr - CLAMP_STD * std_lr\n",
    "\n",
    "df['LogRet'] = np.where(df['LogRet'] > upper, upper,\n",
    "                        np.where(df['LogRet'] < lower, lower, df['LogRet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot(x= 'Date', y='LogRet', title='SP500 Log Returns Over Time', figsize=(15,5))\n",
    "\n",
    "# # Plot histogram of Close prices\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# sns.histplot(df['LogRet'], kde=False, stat=\"density\", bins=30, label='LogRet Prices')\n",
    "\n",
    "# # Fit a normal distribution to the data\n",
    "# mean, std = norm.fit(df['LogRet'])\n",
    "# x = np.linspace(df['LogRet'].min(), df['LogRet'].max(), 100)\n",
    "# pdf = norm.pdf(x, mean, std)\n",
    "\n",
    "# # Plot the normal distribution curve\n",
    "# plt.plot(x, pdf, 'r-', label=f'Normal Fit (mean={mean:.2f}, std={std:.2f})')\n",
    "# plt.title('Distribution of SP500 LogRet Prices')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a collumn calles highlow_range which is the difference between the high and low prices\n",
    "df['highlow_range'] = df['High'] - df['Low']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cordf = df.drop(columns=['Date']).corr()\n",
    "# # cordf = df.drop(columns=['Date']).corr()\n",
    "# cordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correlation plot of all the features in the dataframe\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cordf, annot=True, fmt=\".4f\", cmap='coolwarm', cbar=True, vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix of SP500 Features')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preparation (No global scaling; features exclude Date and LogRet)\n",
    "features = ['LogRet', 'Volume', 'highlow_range']\n",
    "X = df[features].values\n",
    "y = df['LogRet'].shift(-1).dropna().values  # Next LogRet; len N-1\n",
    "X = X[:-1]  # Align; assume df sorted by Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch SRNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit  # For guidance, but implementing custom rolling for financial data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: Define Elman RNN Model\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Step 2: Custom Dataset for Sequences\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, seq_len=10):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.targets[idx + self.seq_len]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "class JordanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Jordan RNN Model. The num_layers parameter is kept for API consistency\n",
    "        but this implementation uses a single hidden layer.\n",
    "        \"\"\"\n",
    "        super(JordanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # The RNN cell takes the input and the combined hidden+previous_output state\n",
    "        self.rnn_cell = nn.RNNCell(input_size + output_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer to produce the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get batch size and sequence length\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Initialize hidden state and output for the first time step\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        prev_output = torch.zeros(batch_size, self.fc.out_features).to(x.device)\n",
    "\n",
    "        # Manually loop through each time step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            # Concatenate current input with the previous time step's output\n",
    "            combined_input = torch.cat((x[:, t, :], prev_output), dim=1)\n",
    "            \n",
    "            # Update the hidden state\n",
    "            hidden = self.rnn_cell(combined_input, hidden)\n",
    "            \n",
    "            # The output for this step is based on the new hidden state\n",
    "            prev_output = self.fc(hidden)\n",
    "            \n",
    "        # The final output is the output from the last time step\n",
    "        return prev_output\n",
    "    \n",
    "class MultiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Multi-Recurrent Neural Network (MultiRNN) - Combination of Elman and Jordan RNNs\n",
    "        \n",
    "        This network feeds back both:\n",
    "        1. Previous hidden state (like Elman RNN)\n",
    "        2. Previous output (like Jordan RNN)\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden units\n",
    "            output_size: Number of output features\n",
    "            num_layers: Number of layers (kept for API consistency, uses single layer)\n",
    "        \"\"\"\n",
    "        super(MultiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # The RNN cell takes input + previous_hidden + previous_output\n",
    "        # Input concatenation: [current_input, prev_hidden, prev_output]\n",
    "        self.rnn_cell = nn.RNNCell(input_size + hidden_size + output_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer to produce the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MultiRNN\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Get batch size and sequence length\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Initialize hidden state and output for the first time step\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        prev_output = torch.zeros(batch_size, self.output_size).to(x.device)\n",
    "\n",
    "        # Manually loop through each time step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            # Concatenate current input, previous hidden state, and previous output\n",
    "            # This is the key difference: we use BOTH prev_hidden and prev_output\n",
    "            combined_input = torch.cat((x[:, t, :], hidden, prev_output), dim=1)\n",
    "            \n",
    "            # Update the hidden state using the combined input\n",
    "            hidden = self.rnn_cell(combined_input, hidden)\n",
    "            \n",
    "            # Calculate the current output based on the new hidden state\n",
    "            current_output = self.fc(hidden)\n",
    "            \n",
    "            # Update prev_output for the next time step\n",
    "            prev_output = current_output\n",
    "            \n",
    "        # Return the final output (output from the last time step)\n",
    "        return current_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters and train validation test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "hidden_size = 100 #Hyperparameter to tune\n",
    "num_layers = 1\n",
    "learning_rate = 0.01 #Hyperparameter to tune\n",
    "epochs = 500\n",
    "n_folds = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(features)\n",
    "output_size = 1\n",
    "\n",
    "model_to_use = 'Elman'  # or Elman\n",
    "\n",
    "# --- New: one overarching train/holdout split at the beginning (holdout is the last chunk of the series)\n",
    "holdout_ratio = 0.3  # fraction kept for final test at the end\n",
    "n_total = len(X)\n",
    "holdout_size = int(holdout_ratio * n_total)\n",
    "if holdout_size < seq_len + 1:\n",
    "    raise ValueError(\"Holdout set too small for the chosen seq_len; reduce holdout_ratio or seq_len.\")\n",
    "\n",
    "# Training and validation portion\n",
    "X_trainval = X[:-holdout_size]\n",
    "y_trainval = y[:-holdout_size]\n",
    "\n",
    "# Held out portion\n",
    "X_holdout = X[-holdout_size:]\n",
    "y_holdout = y[-holdout_size:]\n",
    "\n",
    "# --- Growing-window CV on the training+validation portion\n",
    "results = {'fold': [], 'rmse': [], 'mae': []}\n",
    "\n",
    "n_trainval = len(X_trainval)\n",
    "# initial training window inside trainval (choose a sensible starting window)\n",
    "initial_train_ratio = 0.5\n",
    "initial_train_window = max(int(initial_train_ratio * n_trainval), seq_len + 1)\n",
    "remaining = n_trainval - initial_train_window\n",
    "if remaining < n_folds:  # ensure at least one sample per test fold\n",
    "    raise ValueError(\"Not enough data in training portion to create the requested number of folds. Reduce n_folds or holdout_ratio.\")\n",
    "\n",
    "fold_test_size = max(1, remaining // n_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def gridsearchCV_RNNs(model_to_use='Elman'):\n",
    "\n",
    "    # Step 1: Define the grid of hyperparameters to search over\n",
    "    # Keep this small initially, as grid search is computationally expensive!\n",
    "    param_grid = {\n",
    "        'hidden_size': range(150, 200, 10),\n",
    "        'learning_rate': [0.001, 0.005, 0.01, 0.015, 0.02],  # Use a list of floats instead of range\n",
    "        'seq_len': range(10, 31, 10),\n",
    "    }\n",
    "\n",
    "    # --- Parameters for Early Stopping ---\n",
    "    epochs = 500  # Max number of epochs\n",
    "    patience = 15 # How many epochs to wait for improvement before stopping\n",
    "\n",
    "    # --- Store results for each parameter combination ---\n",
    "    grid_search_results = []\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    # Step 2: Outer loop for Grid Search\n",
    "    for v in itertools.product(*values):\n",
    "        params = dict(zip(keys, v))\n",
    "        \n",
    "        # Unpack current hyperparameters\n",
    "        hidden_size = params['hidden_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        seq_len = params['seq_len']\n",
    "        # You can unpack others like batch_size here if you add them to the grid\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING HYPERPARAMETERS: {params}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Store results for the folds of this specific parameter set\n",
    "        current_param_fold_results = {'rmse': [], 'mae': []}\n",
    "\n",
    "        # Step 3: Inner loop for Growing-Window Cross-Validation (your original code)\n",
    "        for fold in range(n_folds):\n",
    "            train_start = 0\n",
    "            train_end = initial_train_window + fold * fold_test_size\n",
    "            test_start = train_end\n",
    "            test_end = min(test_start + fold_test_size, n_trainval)\n",
    "\n",
    "            # Ensure there's enough data for at least one sequence\n",
    "            if train_end - train_start < seq_len + 1 or test_end - test_start < seq_len + 1:\n",
    "                print(f\"Skipping fold {fold}: Insufficient data for seq_len={seq_len}\")\n",
    "                continue\n",
    "\n",
    "            # Extract fold data\n",
    "            X_tr, y_tr = X_trainval[train_start:train_end], y_trainval[train_start:train_end]\n",
    "            X_te, y_te = X_trainval[test_start:test_end], y_trainval[test_start:test_end]\n",
    "\n",
    "            # Scale PER FOLD\n",
    "            scaler_X = StandardScaler().fit(X_tr)\n",
    "            X_tr_scaled = scaler_X.transform(X_tr)\n",
    "            X_te_scaled = scaler_X.transform(X_te)\n",
    "            scaler_y = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "            y_tr_scaled = scaler_y.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "            y_te_scaled = scaler_y.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Create datasets & loaders with the current seq_len\n",
    "            train_dataset = TimeSeriesDataset(X_tr_scaled, y_tr_scaled, seq_len)\n",
    "            # The test_dataset is now our validation set for early stopping\n",
    "            val_dataset = TimeSeriesDataset(X_te_scaled, y_te_scaled, seq_len) \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize model with current hyperparameters\n",
    "            if model_to_use == 'Jordan':\n",
    "                model = JordanRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "            elif model_to_use == 'Elman':\n",
    "                model = ElmanRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "            elif model_to_use == 'Multi':\n",
    "                model = MultiRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model_to_use specified.\")\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # --- Training with Early Stopping ---\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Validation phase for early stopping\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_x, batch_y in val_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_x)\n",
    "                        val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
    "                \n",
    "                val_loss /= len(val_loader)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Optional: save the best model state\n",
    "                    torch.save(model.state_dict(), f'{model_to_use}_best_model_fold.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    # print(f\"Fold {fold}: Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            # Load the best model for evaluation\n",
    "            model.load_state_dict(torch.load(f'{model_to_use}_best_model_fold.pth'))\n",
    "\n",
    "            # Evaluation on this fold's test block\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_x, _ in val_loader: # Use the same validation loader\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    outputs = model(batch_x)\n",
    "                    predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "            predictions = np.array(predictions)\n",
    "            true_y = y_te_scaled[seq_len:]\n",
    "            predictions = predictions[:len(true_y)]\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "            mae = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "            current_param_fold_results['rmse'].append(rmse)\n",
    "            current_param_fold_results['mae'].append(mae)\n",
    "\n",
    "        # After all folds for the current param set are done, calculate the average score\n",
    "        if current_param_fold_results['rmse']: # Check if any folds ran\n",
    "            mean_rmse = np.mean(current_param_fold_results['rmse'])\n",
    "            mean_mae = np.mean(current_param_fold_results['mae'])\n",
    "            \n",
    "            # print(f\"\\n--- Avg CV Score for {params}: RMSE = {mean_rmse:.4f}, MAE = {mean_mae:.4f} ---\")\n",
    "            \n",
    "            grid_search_results.append({\n",
    "                'params': params,\n",
    "                'mean_rmse': mean_rmse,\n",
    "                'mean_mae': mean_mae\n",
    "            })\n",
    "\n",
    "    # Step 4: Find and print the best hyperparameter combination\n",
    "    if grid_search_results:\n",
    "        best_params_result = min(grid_search_results, key=lambda x: x['mean_rmse'])\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"GRID SEARCH COMPLETE for model:\", model_to_use)\n",
    "        print(f\"Best Hyperparameters found: {best_params_result['params']}\")\n",
    "        print(f\"Best Mean CV RMSE: {best_params_result['mean_rmse']:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        return best_params_result\n",
    "    else:\n",
    "        print(\"\\nGrid search did not complete. Check data size and parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Xh = X_holdout  # numpy array from your code\n",
    "print(\"shape:\", Xh.shape)\n",
    "print(\"any inf:\", np.isinf(Xh).any())\n",
    "print(\"any nan:\", np.isnan(Xh).any())\n",
    "finite_mask = np.isfinite(Xh)\n",
    "print(\"finite all?:\", finite_mask.all())\n",
    "# extremes\n",
    "finite_vals = Xh[finite_mask]\n",
    "print(\"max abs finite value:\", np.nan if finite_vals.size==0 else np.max(np.abs(finite_vals)))\n",
    "# if you still use df/feature names:\n",
    "Xh_df = pd.DataFrame(Xh, columns=features)\n",
    "print(Xh_df.replace([np.inf, -np.inf], np.nan).isna().sum())\n",
    "\n",
    "# convert inf -> nan\n",
    "X_trainval = np.where(np.isfinite(X_trainval), X_trainval, np.nan)\n",
    "X_holdout  = np.where(np.isfinite(X_holdout),  X_holdout,  np.nan)\n",
    "\n",
    "# impute (median is robust)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_trainval = imp.fit_transform(X_trainval)\n",
    "X_holdout  = imp.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing final testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Final: retrain on entire trainval and evaluate once on the held-out end-of-series test set\n",
    "# Fit scalers on whole trainval\n",
    "scaler_X_full = StandardScaler().fit(X_trainval)\n",
    "X_trainval_scaled = scaler_X_full.transform(X_trainval)\n",
    "X_holdout_scaled = scaler_X_full.transform(X_holdout)\n",
    "\n",
    "scaler_y_full = StandardScaler().fit(y_trainval.reshape(-1, 1))\n",
    "y_trainval_scaled = scaler_y_full.transform(y_trainval.reshape(-1, 1)).flatten()\n",
    "y_holdout_scaled = scaler_y_full.transform(y_holdout.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Elman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_result = gridsearchCV_RNNs(model_to_use='Elman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on entire trainval\n",
    "train_dataset_full = TimeSeriesDataset(X_trainval_scaled, y_trainval_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "holdout_dataset = TimeSeriesDataset(X_holdout_scaled, y_holdout_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=False)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "final_model = ElmanRNN(input_size, best_params_result['params']['hidden_size'], output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params_result['params']['learning_rate'])\n",
    "final_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader_full:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on holdout\n",
    "final_model.eval()\n",
    "predictions = []\n",
    "true_vals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in holdout_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = final_model(batch_x)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_vals.extend(batch_y.numpy().flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_y = np.array(true_vals)\n",
    "\n",
    "rmse_holdout = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "mae_holdout = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "print(f\"\\nFinal Holdout Test (on last {holdout_size} samples): RMSE = {rmse_holdout:.4f}, MAE = {mae_holdout:.4f}\")\n",
    "\n",
    "# Optional: inverse-transform to original y scale for more interpretable metrics\n",
    "y_pred_inv = scaler_y_full.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "y_true_inv = scaler_y_full.inverse_transform(true_y.reshape(-1, 1)).flatten()\n",
    "rmse_holdout_orig = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "mae_holdout_orig = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "print(f\"Final Holdout Test (original y scale): RMSE = {rmse_holdout_orig:.6f}, MAE = {mae_holdout_orig:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Jordan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_result = gridsearchCV_RNNs(model_to_use='Jordan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on entire trainval\n",
    "train_dataset_full = TimeSeriesDataset(X_trainval_scaled, y_trainval_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "holdout_dataset = TimeSeriesDataset(X_holdout_scaled, y_holdout_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=False)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "final_model = JordanRNN(input_size, best_params_result['params']['hidden_size'], output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params_result['params']['learning_rate'])\n",
    "final_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader_full:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on holdout\n",
    "final_model.eval()\n",
    "predictions = []\n",
    "true_vals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in holdout_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = final_model(batch_x)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_vals.extend(batch_y.numpy().flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_y = np.array(true_vals)\n",
    "\n",
    "rmse_holdout = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "mae_holdout = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "print(f\"\\nFinal Holdout Test (on last {holdout_size} samples): RMSE = {rmse_holdout:.4f}, MAE = {mae_holdout:.4f}\")\n",
    "\n",
    "# Optional: inverse-transform to original y scale for more interpretable metrics\n",
    "y_pred_inv = scaler_y_full.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "y_true_inv = scaler_y_full.inverse_transform(true_y.reshape(-1, 1)).flatten()\n",
    "rmse_holdout_orig = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "mae_holdout_orig = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "print(f\"Final Holdout Test (original y scale): RMSE = {rmse_holdout_orig:.6f}, MAE = {mae_holdout_orig:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Multi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_result = gridsearchCV_RNNs(model_to_use='Multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_full = TimeSeriesDataset(X_trainval_scaled, y_trainval_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "holdout_dataset = TimeSeriesDataset(X_holdout_scaled, y_holdout_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=False)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train final model on entire trainval\n",
    "final_model = MultiRNN(input_size, best_params_result['params']['hidden_size'], output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params_result['params']['learning_rate'])\n",
    "final_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader_full:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on holdout\n",
    "final_model.eval()\n",
    "predictions = []\n",
    "true_vals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in holdout_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = final_model(batch_x)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_vals.extend(batch_y.numpy().flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_y = np.array(true_vals)\n",
    "\n",
    "rmse_holdout = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "mae_holdout = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "print(f\"\\nFinal Holdout Test (on last {holdout_size} samples): RMSE = {rmse_holdout:.4f}, MAE = {mae_holdout:.4f}\")\n",
    "\n",
    "# Optional: inverse-transform to original y scale for more interpretable metrics\n",
    "y_pred_inv = scaler_y_full.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "y_true_inv = scaler_y_full.inverse_transform(true_y.reshape(-1, 1)).flatten()\n",
    "rmse_holdout_orig = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "mae_holdout_orig = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "print(f\"Final Holdout Test (original y scale): RMSE = {rmse_holdout_orig:.6f}, MAE = {mae_holdout_orig:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
