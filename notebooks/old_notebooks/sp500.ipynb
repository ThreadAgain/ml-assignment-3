{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>LogRet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-09-20</td>\n",
       "      <td>1221.339966</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>-0.007895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-09-21</td>\n",
       "      <td>1210.199951</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.008379</td>\n",
       "      <td>-0.007895</td>\n",
       "      <td>0.094124</td>\n",
       "      <td>-0.009163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-09-22</td>\n",
       "      <td>1214.619995</td>\n",
       "      <td>-0.004003</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>-0.009163</td>\n",
       "      <td>-0.049652</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-09-23</td>\n",
       "      <td>1215.290039</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>-0.206151</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-09-26</td>\n",
       "      <td>1215.630005</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>6587.470215</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.008450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>6584.290039</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>-0.156219</td>\n",
       "      <td>-0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>6615.279785</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.004696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>2025-09-16</td>\n",
       "      <td>6606.759766</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.060471</td>\n",
       "      <td>-0.001289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>6600.350098</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>-0.007446</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>0.079906</td>\n",
       "      <td>-0.000971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5030 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Close      High       Low      Open    Volume  \\\n",
       "0     2005-09-20  1221.339966 -0.001148 -0.006194 -0.005581  0.110541   \n",
       "1     2005-09-21  1210.199951 -0.012181 -0.008379 -0.007895  0.094124   \n",
       "2     2005-09-22  1214.619995 -0.004003 -0.003759 -0.009163 -0.049652   \n",
       "3     2005-09-23  1215.290039  0.001798  0.003685  0.003646 -0.206151   \n",
       "4     2005-09-26  1215.630005  0.003056  0.001685  0.000551  0.024631   \n",
       "...          ...          ...       ...       ...       ...       ...   \n",
       "5025  2025-09-11  6587.470215  0.005616  0.004511  0.000629  0.032486   \n",
       "5026  2025-09-12  6584.290039  0.001110  0.005134  0.005515 -0.156219   \n",
       "5027  2025-09-15  6615.279785  0.002937  0.003426  0.001945  0.083334   \n",
       "5028  2025-09-16  6606.759766  0.001113 -0.000297  0.003121  0.060471   \n",
       "5029  2025-09-17  6600.350098 -0.000392 -0.007446 -0.002912  0.079906   \n",
       "\n",
       "        LogRet  \n",
       "0    -0.007895  \n",
       "1    -0.009163  \n",
       "2     0.003646  \n",
       "3     0.000551  \n",
       "4     0.000280  \n",
       "...        ...  \n",
       "5025  0.008450  \n",
       "5026 -0.000483  \n",
       "5027  0.004696  \n",
       "5028 -0.001289  \n",
       "5029 -0.000971  \n",
       "\n",
       "[5030 rows x 7 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/cleaned/sp500.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df.dropna(inplace=True)\n",
    "# Plot histogram of Close prices\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(df['LogRet'], kde=False, stat=\"density\", bins=30, label='LogRet Prices')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mean, std = norm.fit(df['LogRet'])\n",
    "x = np.linspace(df['LogRet'].min(), df['LogRet'].max(), 100)\n",
    "pdf = norm.pdf(x, mean, std)\n",
    "\n",
    "# Plot the normal distribution curve\n",
    "plt.plot(x, pdf, 'r-', label=f'Normal Fit (mean={mean:.2f}, std={std:.2f})')\n",
    "plt.title('Distribution of SP500 LogRet Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STATIONARITY ANALYSIS: S&P500_RAW\n",
      "==================================================\n",
      "\n",
      "--- AUGMENTED DICKEY-FULLER TEST ---\n",
      "ADF Statistic: 2.214788\n",
      "p-value: 0.998894\n",
      "Critical Values:\n",
      "\t1%: -3.431658\n",
      "\t5%: -2.862118\n",
      "\t10%: -2.567078\n",
      "ADF Result: NON-STATIONARY (Fail to reject null hypothesis)\n",
      "\n",
      "--- KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN TEST ---\n",
      "KPSS Statistic: 10.284780\n",
      "p-value: 0.010000\n",
      "Critical Values:\n",
      "\t10%: 0.347000\n",
      "\t5%: 0.463000\n",
      "\t2.5%: 0.574000\n",
      "\t1%: 0.739000\n",
      "KPSS Result: NON-STATIONARY (Reject null hypothesis)\n",
      "\n",
      "--- COMBINED INTERPRETATION ---\n",
      "CONCLUSION: Series is NON-STATIONARY\n",
      "\n",
      "==================================================\n",
      "STATIONARITY ANALYSIS: S&P500_LOG_RETURNS\n",
      "==================================================\n",
      "\n",
      "--- AUGMENTED DICKEY-FULLER TEST ---\n",
      "ADF Statistic: -17.945452\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.431655\n",
      "\t5%: -2.862117\n",
      "\t10%: -2.567077\n",
      "ADF Result: STATIONARY (Reject null hypothesis)\n",
      "\n",
      "--- KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN TEST ---\n",
      "KPSS Statistic: 0.189312\n",
      "p-value: 0.100000\n",
      "Critical Values:\n",
      "\t10%: 0.347000\n",
      "\t5%: 0.463000\n",
      "\t2.5%: 0.574000\n",
      "\t1%: 0.739000\n",
      "KPSS Result: STATIONARY (Fail to reject null hypothesis)\n",
      "\n",
      "--- COMBINED INTERPRETATION ---\n",
      "CONCLUSION: Series is STATIONARY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def stationarity_analysis(df, dataset_name, column_name = None):\n",
    "    \"\"\"\n",
    "    column_name: specify the column to analyze, default is 'Close'\n",
    "    Perform ADF and KPSS tests for stationarity analysis\n",
    "    \"\"\"\n",
    "    data = df['Close'] if column_name is None else df[column_name]\n",
    "    \n",
    "    data.dropna(inplace=True)  # Drop NaN values for accurate testing\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"STATIONARITY ANALYSIS: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # ADF Test\n",
    "    print(\"\\n--- AUGMENTED DICKEY-FULLER TEST ---\")\n",
    "    adf_result = adfuller(data, autolag='AIC')\n",
    "    print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
    "    print(f\"p-value: {adf_result[1]:.6f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"\\t{key}: {value:.6f}\")\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"ADF Result: STATIONARY (Reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"ADF Result: NON-STATIONARY (Fail to reject null hypothesis)\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    print(\"\\n--- KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN TEST ---\")\n",
    "    kpss_result = kpss(data, regression='c', nlags='auto')\n",
    "    print(f\"KPSS Statistic: {kpss_result[0]:.6f}\")\n",
    "    print(f\"p-value: {kpss_result[1]:.6f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f\"\\t{key}: {value:.6f}\")\n",
    "    \n",
    "    if kpss_result[1] <= 0.05:\n",
    "        print(\"KPSS Result: NON-STATIONARY (Reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"KPSS Result: STATIONARY (Fail to reject null hypothesis)\")\n",
    "    \n",
    "    # Combined interpretation\n",
    "    print(\"\\n--- COMBINED INTERPRETATION ---\")\n",
    "    adf_stationary = adf_result[1] <= 0.05\n",
    "    kpss_stationary = kpss_result[1] > 0.05\n",
    "    \n",
    "    if adf_stationary and kpss_stationary:\n",
    "        print(\"CONCLUSION: Series is STATIONARY\")\n",
    "    elif not adf_stationary and not kpss_stationary:\n",
    "        print(\"CONCLUSION: Series is NON-STATIONARY\")\n",
    "    else:\n",
    "        print(\"CONCLUSION: Results are INCONCLUSIVE - further investigation needed\")\n",
    "\n",
    "stationarity_analysis(df=df, dataset_name=\"S&P500_raw\")\n",
    "stationarity_analysis(df=df, dataset_name=\"S&P500_log_returns\", column_name='LogRet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr = df['LogRet'].mean()\n",
    "std_lr = df['LogRet'].std()\n",
    "CLAMP_STD = 3\n",
    "\n",
    "upper = mean_lr + CLAMP_STD * std_lr\n",
    "lower = mean_lr - CLAMP_STD * std_lr\n",
    "\n",
    "df['LogRet'] = np.where(df['LogRet'] > upper, upper,\n",
    "                        np.where(df['LogRet'] < lower, lower, df['LogRet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot(x= 'Date', y='LogRet', title='SP500 Log Returns Over Time', figsize=(15,5))\n",
    "\n",
    "# # Plot histogram of Close prices\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# sns.histplot(df['LogRet'], kde=False, stat=\"density\", bins=30, label='LogRet Prices')\n",
    "\n",
    "# # Fit a normal distribution to the data\n",
    "# mean, std = norm.fit(df['LogRet'])\n",
    "# x = np.linspace(df['LogRet'].min(), df['LogRet'].max(), 100)\n",
    "# pdf = norm.pdf(x, mean, std)\n",
    "\n",
    "# # Plot the normal distribution curve\n",
    "# plt.plot(x, pdf, 'r-', label=f'Normal Fit (mean={mean:.2f}, std={std:.2f})')\n",
    "# plt.title('Distribution of SP500 LogRet Prices')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a collumn calles highlow_range which is the difference between the high and low prices\n",
    "df['highlow_range'] = df['High'] - df['Low']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cordf = df.drop(columns=['Date']).corr()\n",
    "# # cordf = df.drop(columns=['Date']).corr()\n",
    "# cordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correlation plot of all the features in the dataframe\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cordf, annot=True, fmt=\".4f\", cmap='coolwarm', cbar=True, vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix of SP500 Features')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the PyTorch sRNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit  # For guidance, but implementing custom rolling for financial data\n",
    "\n",
    "# Step 1: Define Elman RNN Model\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Step 2: Custom Dataset for Sequences\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, seq_len=10):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.targets[idx + self.seq_len]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "# Step 3: Data Preparation (No global scaling; features exclude Date and LogRet)\n",
    "features = ['LogRet', 'Volume', 'highlow_range']\n",
    "X = df[features].values\n",
    "y = df['LogRet'].shift(-1).dropna().values  # Next LogRet; len N-1\n",
    "X = X[:-1]  # Align; assume df sorted by Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JordanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Jordan RNN Model. The num_layers parameter is kept for API consistency\n",
    "        but this implementation uses a single hidden layer.\n",
    "        \"\"\"\n",
    "        super(JordanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # The RNN cell takes the input and the combined hidden+previous_output state\n",
    "        self.rnn_cell = nn.RNNCell(input_size + output_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer to produce the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get batch size and sequence length\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Initialize hidden state and output for the first time step\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        prev_output = torch.zeros(batch_size, self.fc.out_features).to(x.device)\n",
    "\n",
    "        # Manually loop through each time step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            # Concatenate current input with the previous time step's output\n",
    "            combined_input = torch.cat((x[:, t, :], prev_output), dim=1)\n",
    "            \n",
    "            # Update the hidden state\n",
    "            hidden = self.rnn_cell(combined_input, hidden)\n",
    "            \n",
    "            # The output for this step is based on the new hidden state\n",
    "            prev_output = self.fc(hidden)\n",
    "            \n",
    "        # The final output is the output from the last time step\n",
    "        return prev_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters and train validation test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "hidden_size = 100 #Hyperparameter to tune\n",
    "num_layers = 1\n",
    "learning_rate = 0.01 #Hyperparameter to tune\n",
    "epochs = 500\n",
    "n_folds = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(features)\n",
    "output_size = 1\n",
    "\n",
    "model_to_use = 'Jordan'  # or Elman\n",
    "\n",
    "# --- New: one overarching train/holdout split at the beginning (holdout is the last chunk of the series)\n",
    "holdout_ratio = 0.3  # fraction kept for final test at the end\n",
    "n_total = len(X)\n",
    "holdout_size = int(holdout_ratio * n_total)\n",
    "if holdout_size < seq_len + 1:\n",
    "    raise ValueError(\"Holdout set too small for the chosen seq_len; reduce holdout_ratio or seq_len.\")\n",
    "\n",
    "# Training and validation portion\n",
    "X_trainval = X[:-holdout_size]\n",
    "y_trainval = y[:-holdout_size]\n",
    "\n",
    "# Held out portion\n",
    "X_holdout = X[-holdout_size:]\n",
    "y_holdout = y[-holdout_size:]\n",
    "\n",
    "# --- Growing-window CV on the training+validation portion\n",
    "results = {'fold': [], 'rmse': [], 'mae': []}\n",
    "\n",
    "n_trainval = len(X_trainval)\n",
    "# initial training window inside trainval (choose a sensible starting window)\n",
    "initial_train_ratio = 0.5\n",
    "initial_train_window = max(int(initial_train_ratio * n_trainval), seq_len + 1)\n",
    "remaining = n_trainval - initial_train_window\n",
    "if remaining < n_folds:  # ensure at least one sample per test fold\n",
    "    raise ValueError(\"Not enough data in training portion to create the requested number of folds. Reduce n_folds or holdout_ratio.\")\n",
    "\n",
    "fold_test_size = max(1, remaining // n_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle data issues from the train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1508, 3)\n",
      "any inf: True\n",
      "any nan: False\n",
      "finite all?: False\n",
      "max abs finite value: 0.8602027484710865\n",
      "LogRet           0\n",
      "Volume           2\n",
      "highlow_range    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Xh = X_holdout  # numpy array from your code\n",
    "print(\"shape:\", Xh.shape)\n",
    "print(\"any inf:\", np.isinf(Xh).any())\n",
    "print(\"any nan:\", np.isnan(Xh).any())\n",
    "finite_mask = np.isfinite(Xh)\n",
    "print(\"finite all?:\", finite_mask.all())\n",
    "# extremes\n",
    "finite_vals = Xh[finite_mask]\n",
    "print(\"max abs finite value:\", np.nan if finite_vals.size==0 else np.max(np.abs(finite_vals)))\n",
    "# if you still use df/feature names:\n",
    "Xh_df = pd.DataFrame(Xh, columns=features)\n",
    "print(Xh_df.replace([np.inf, -np.inf], np.nan).isna().sum())\n",
    "\n",
    "# convert inf -> nan\n",
    "X_trainval = np.where(np.isfinite(X_trainval), X_trainval, np.nan)\n",
    "X_holdout  = np.where(np.isfinite(X_holdout),  X_holdout,  np.nan)\n",
    "\n",
    "# impute (median is robust)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_trainval = imp.fit_transform(X_trainval)\n",
    "X_holdout  = imp.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for fold in range(n_folds):\n",
    "#     train_start = 0\n",
    "#     train_end = initial_train_window + fold * fold_test_size  # growing window\n",
    "#     test_start = train_end\n",
    "#     test_end = min(test_start + fold_test_size, n_trainval)\n",
    "\n",
    "#     if train_end - train_start < seq_len + 1 or test_end - test_start < seq_len + 1:\n",
    "#         print(f\"Skipping fold {fold}: Insufficient data (train {train_start}-{train_end}, test {test_start}-{test_end})\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract fold data (relative to trainval)\n",
    "#     X_tr = X_trainval[train_start:train_end]\n",
    "#     y_tr = y_trainval[train_start:train_end]\n",
    "#     X_te = X_trainval[test_start:test_end]\n",
    "#     y_te = y_trainval[test_start:test_end]\n",
    "\n",
    "#     # Scale PER FOLD (fit on train only to avoid leakage)\n",
    "#     scaler_X = StandardScaler().fit(X_tr)\n",
    "#     X_tr_scaled = scaler_X.transform(X_tr)\n",
    "#     X_te_scaled = scaler_X.transform(X_te)\n",
    "\n",
    "#     scaler_y = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "#     y_tr_scaled = scaler_y.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "#     y_te_scaled = scaler_y.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "#     # Create datasets/loaders\n",
    "#     train_dataset = TimeSeriesDataset(X_tr_scaled, y_tr_scaled, seq_len)\n",
    "#     test_dataset = TimeSeriesDataset(X_te_scaled, y_te_scaled, seq_len)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Model, loss, optimizer (fresh model per fold)\n",
    "#     model = ElmanRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         for batch_x, batch_y in train_loader:\n",
    "#             batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(batch_x)\n",
    "#             loss = criterion(outputs.squeeze(), batch_y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     # Evaluation on this fold's test block\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch_x, _ in test_loader:\n",
    "#             batch_x = batch_x.to(device)\n",
    "#             outputs = model(batch_x)\n",
    "#             predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "#     predictions = np.array(predictions)\n",
    "#     true_y = y_te_scaled[seq_len:]  # align with sequence outputs\n",
    "#     predictions = predictions[:len(true_y)]\n",
    "\n",
    "#     rmse = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "#     mae = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "#     results['fold'].append(fold)\n",
    "#     results['rmse'].append(rmse)\n",
    "#     results['mae'].append(mae)\n",
    "\n",
    "#     print(f\"Fold {fold}: RMSE = {rmse:.4f}, MAE = {mae:.4f} (Train: {train_start}-{train_end}, Test: {test_start}-{test_end})\")\n",
    "\n",
    "# # Summarize CV results\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(\"\\nGrowing-window CV Summary (on train+val):\")\n",
    "# print(results_df)\n",
    "# if not results_df.empty:\n",
    "#     print(f\"Mean RMSE: {results_df['rmse'].mean():.4f} (+/- {results_df['rmse'].std() * 2:.4f})\")\n",
    "#     print(f\"Mean MAE: {results_df['mae'].mean():.4f} (+/- {results_df['mae'].std() * 2:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.001, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 61\n",
      "Fold 1: Early stopping at epoch 63\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.001, 'seq_len': 10}: RMSE = 0.6803, MAE = 0.4779 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.001, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 56\n",
      "Fold 1: Early stopping at epoch 53\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.001, 'seq_len': 20}: RMSE = 0.6805, MAE = 0.4781 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.01, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 68\n",
      "Fold 1: Early stopping at epoch 54\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.01, 'seq_len': 10}: RMSE = 0.6783, MAE = 0.4764 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.01, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 56\n",
      "Fold 1: Early stopping at epoch 52\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.01, 'seq_len': 20}: RMSE = 0.6783, MAE = 0.4759 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.015, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 116\n",
      "Fold 1: Early stopping at epoch 63\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.015, 'seq_len': 10}: RMSE = 0.6776, MAE = 0.4753 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 50, 'learning_rate': 0.015, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 51\n",
      "Fold 1: Early stopping at epoch 62\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 50, 'learning_rate': 0.015, 'seq_len': 20}: RMSE = 0.6785, MAE = 0.4760 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.001, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 61\n",
      "Fold 1: Early stopping at epoch 66\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.001, 'seq_len': 10}: RMSE = 0.6794, MAE = 0.4775 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.001, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 75\n",
      "Fold 1: Early stopping at epoch 54\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.001, 'seq_len': 20}: RMSE = 0.6809, MAE = 0.4782 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.01, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 57\n",
      "Fold 1: Early stopping at epoch 55\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.01, 'seq_len': 10}: RMSE = 0.6777, MAE = 0.4758 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.01, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 58\n",
      "Fold 1: Early stopping at epoch 58\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.01, 'seq_len': 20}: RMSE = 0.6795, MAE = 0.4769 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.015, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 53\n",
      "Fold 1: Early stopping at epoch 57\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.015, 'seq_len': 10}: RMSE = 0.6896, MAE = 0.4910 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 100, 'learning_rate': 0.015, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 84\n",
      "Fold 1: Early stopping at epoch 111\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 100, 'learning_rate': 0.015, 'seq_len': 20}: RMSE = 0.6787, MAE = 0.4765 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.001, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 85\n",
      "Fold 1: Early stopping at epoch 64\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.001, 'seq_len': 10}: RMSE = 0.6801, MAE = 0.4780 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.001, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 100\n",
      "Fold 1: Early stopping at epoch 59\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.001, 'seq_len': 20}: RMSE = 0.6807, MAE = 0.4786 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.01, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 69\n",
      "Fold 1: Early stopping at epoch 59\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.01, 'seq_len': 10}: RMSE = 0.6771, MAE = 0.4754 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.01, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 63\n",
      "Fold 1: Early stopping at epoch 51\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.01, 'seq_len': 20}: RMSE = 0.6801, MAE = 0.4771 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.015, 'seq_len': 10}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 52\n",
      "Fold 1: Early stopping at epoch 90\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.015, 'seq_len': 10}: RMSE = 0.6787, MAE = 0.4767 ---\n",
      "\n",
      "============================================================\n",
      "TESTING HYPERPARAMETERS: {'hidden_size': 150, 'learning_rate': 0.015, 'seq_len': 20}\n",
      "============================================================\n",
      "Fold 0: Early stopping at epoch 80\n",
      "Fold 1: Early stopping at epoch 76\n",
      "\n",
      "--- Avg CV Score for {'hidden_size': 150, 'learning_rate': 0.015, 'seq_len': 20}: RMSE = 0.6785, MAE = 0.4762 ---\n",
      "\n",
      "============================================================\n",
      "GRID SEARCH COMPLETE\n",
      "Best Hyperparameters found: {'hidden_size': 150, 'learning_rate': 0.01, 'seq_len': 10}\n",
      "Best Mean CV RMSE: 0.6771\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Step 1: Define the grid of hyperparameters to search over\n",
    "# Keep this small initially, as grid search is computationally expensive!\n",
    "param_grid = {\n",
    "    # 'hidden_size': range(50, 200, 40),\n",
    "    'hidden_size': [50, 100, 150],  # Use a list of integers instead of range\n",
    "    # 'hidden_size': [50],  # Use a list of integers instead of range\n",
    "    # 'learning_rate': [0.01, 0.001],\n",
    "    'learning_rate': [0.001, 0.01, 0.015],  # Use a list of floats instead of range\n",
    "    # 'learning_rate': [0.015],  # Use a list of floats instead of range\n",
    "    # 'seq_len': range(15, 31, 5),\n",
    "    'seq_len': [10, 20],\n",
    "    # 'seq_len': [20],\n",
    "}\n",
    "\n",
    "# --- Parameters for Early Stopping ---\n",
    "epochs = 500  # Max number of epochs\n",
    "patience = 25 # How many epochs to wait for improvement before stopping\n",
    "\n",
    "# --- Store results for each parameter combination ---\n",
    "grid_search_results = []\n",
    "keys, values = zip(*param_grid.items())\n",
    "\n",
    "# Step 2: Outer loop for Grid Search\n",
    "for v in itertools.product(*values):\n",
    "    params = dict(zip(keys, v))\n",
    "    \n",
    "    # Unpack current hyperparameters\n",
    "    hidden_size = params['hidden_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    seq_len = params['seq_len']\n",
    "    # You can unpack others like batch_size here if you add them to the grid\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING HYPERPARAMETERS: {params}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Store results for the folds of this specific parameter set\n",
    "    current_param_fold_results = {'rmse': [], 'mae': []}\n",
    "\n",
    "    # Step 3: Inner loop for Growing-Window Cross-Validation (your original code)\n",
    "    for fold in range(n_folds):\n",
    "        train_start = 0\n",
    "        train_end = initial_train_window + fold * fold_test_size\n",
    "        test_start = train_end\n",
    "        test_end = min(test_start + fold_test_size, n_trainval)\n",
    "\n",
    "        # Ensure there's enough data for at least one sequence\n",
    "        if train_end - train_start < seq_len + 1 or test_end - test_start < seq_len + 1:\n",
    "            print(f\"Skipping fold {fold}: Insufficient data for seq_len={seq_len}\")\n",
    "            continue\n",
    "\n",
    "        # Extract fold data\n",
    "        X_tr, y_tr = X_trainval[train_start:train_end], y_trainval[train_start:train_end]\n",
    "        X_te, y_te = X_trainval[test_start:test_end], y_trainval[test_start:test_end]\n",
    "\n",
    "        # Scale PER FOLD\n",
    "        scaler_X = StandardScaler().fit(X_tr)\n",
    "        X_tr_scaled = scaler_X.transform(X_tr)\n",
    "        X_te_scaled = scaler_X.transform(X_te)\n",
    "        scaler_y = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "        y_tr_scaled = scaler_y.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "        y_te_scaled = scaler_y.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Create datasets & loaders with the current seq_len\n",
    "        train_dataset = TimeSeriesDataset(X_tr_scaled, y_tr_scaled, seq_len)\n",
    "        # The test_dataset is now our validation set for early stopping\n",
    "        val_dataset = TimeSeriesDataset(X_te_scaled, y_te_scaled, seq_len) \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model with current hyperparameters\n",
    "        if model_to_use == 'Jordan':\n",
    "            model = JordanRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "        elif model_to_use == 'Elman':\n",
    "            model = ElmanRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "        elif model_to_use == 'Multi':\n",
    "            print(\"Multi-RNN not implemented yet.\")\n",
    "            raise ValueError(\"Invalid model_to_use specified.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_to_use specified.\")\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # --- Training with Early Stopping ---\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation phase for early stopping\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    outputs = model(batch_x)\n",
    "                    val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Optional: save the best model state\n",
    "                torch.save(model.state_dict(), 'best_model_fold.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Fold {fold}: Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load the best model for evaluation\n",
    "        model.load_state_dict(torch.load('best_model_fold.pth'))\n",
    "\n",
    "        # Evaluation on this fold's test block\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in val_loader: # Use the same validation loader\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        true_y = y_te_scaled[seq_len:]\n",
    "        predictions = predictions[:len(true_y)]\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "        mae = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "        current_param_fold_results['rmse'].append(rmse)\n",
    "        current_param_fold_results['mae'].append(mae)\n",
    "\n",
    "    # After all folds for the current param set are done, calculate the average score\n",
    "    if current_param_fold_results['rmse']: # Check if any folds ran\n",
    "        mean_rmse = np.mean(current_param_fold_results['rmse'])\n",
    "        mean_mae = np.mean(current_param_fold_results['mae'])\n",
    "        \n",
    "        print(f\"\\n--- Avg CV Score for {params}: RMSE = {mean_rmse:.4f}, MAE = {mean_mae:.4f} ---\")\n",
    "        \n",
    "        grid_search_results.append({\n",
    "            'params': params,\n",
    "            'mean_rmse': mean_rmse,\n",
    "            'mean_mae': mean_mae\n",
    "        })\n",
    "\n",
    "# Step 4: Find and print the best hyperparameter combination\n",
    "if grid_search_results:\n",
    "    best_params_result = min(grid_search_results, key=lambda x: x['mean_rmse'])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GRID SEARCH COMPLETE\")\n",
    "    print(f\"Best Hyperparameters found: {best_params_result['params']}\")\n",
    "    print(f\"Best Mean CV RMSE: {best_params_result['mean_rmse']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"\\nGrid search did not complete. Check data size and parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_result = min(grid_search_results, key=lambda x: x['mean_rmse'])\n",
    "best_params_result['params']['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with best parameters and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Final: retrain on entire trainval and evaluate once on the held-out end-of-series test set\n",
    "# Fit scalers on whole trainval\n",
    "scaler_X_full = StandardScaler().fit(X_trainval)\n",
    "X_trainval_scaled = scaler_X_full.transform(X_trainval)\n",
    "X_holdout_scaled = scaler_X_full.transform(X_holdout)\n",
    "\n",
    "scaler_y_full = StandardScaler().fit(y_trainval.reshape(-1, 1))\n",
    "y_trainval_scaled = scaler_y_full.transform(y_trainval.reshape(-1, 1)).flatten()\n",
    "y_holdout_scaled = scaler_y_full.transform(y_holdout.reshape(-1, 1)).flatten()\n",
    "\n",
    "train_dataset_full = TimeSeriesDataset(X_trainval_scaled, y_trainval_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "holdout_dataset = TimeSeriesDataset(X_holdout_scaled, y_holdout_scaled,  seq_len=best_params_result['params']['seq_len'])\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=False)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train final model on entire trainval\n",
    "final_model = ElmanRNN(input_size, best_params_result['params']['hidden_size'], output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params_result['params']['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Holdout Test (on last 1508 samples): RMSE = 1.0602, MAE = 0.7635\n",
      "Final Holdout Test (original y scale): RMSE = 0.011256, MAE = 0.008105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader_full:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on holdout\n",
    "final_model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, _ in holdout_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = final_model(batch_x)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_y = y_holdout_scaled[seq_len:]\n",
    "predictions = predictions[:len(true_y)]\n",
    "\n",
    "rmse_holdout = np.sqrt(mean_squared_error(true_y, predictions))\n",
    "mae_holdout = mean_absolute_error(true_y, predictions)\n",
    "\n",
    "print(f\"\\nFinal Holdout Test (on last {holdout_size} samples): RMSE = {rmse_holdout:.4f}, MAE = {mae_holdout:.4f}\")\n",
    "\n",
    "# Optional: inverse-transform to original y scale for more interpretable metrics\n",
    "y_pred_inv = scaler_y_full.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "y_true_inv = scaler_y_full.inverse_transform(true_y.reshape(-1, 1)).flatten()\n",
    "rmse_holdout_orig = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "mae_holdout_orig = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "print(f\"Final Holdout Test (original y scale): RMSE = {rmse_holdout_orig:.6f}, MAE = {mae_holdout_orig:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
